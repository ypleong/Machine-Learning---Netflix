# Solution set for CS 155 Set 6, 2016/2017
# Authors: Fabian Boemer, Sid Murching, Suraj Nair

import sys
import numpy as np

def grad_U(Ui, Yij, Vj, reg, eta):
    return (1-reg*eta)*Ui + eta * Vj * (Yij - np.dot(Ui,Vj))

def grad_V(Vj, Yij, Ui, reg, eta):
    return (1-reg*eta)*Vj + eta * Ui * (Yij - np.dot(Ui,Vj))

def get_err(U, V, Y):
    err = 0.0
    for (i,j,Yij) in Y:
        err += 0.5 *(Yij - np.dot(U[i-1], V[:,j-1]))**2
    return err / float(len(Y))

def train_model(M, N, K, eta, reg, Y, eps=0.0001, max_epochs=2):
    U = np.random.random((M,K)) - 0.5
    V = np.random.random((K,N)) - 0.5
    size = Y.shape[0]
    delta = None
    print("training reg = %s, k = %s, M = %s, N = %s"%(reg, K, M, N))
    indices = range(size)
    for epoch in range(max_epochs):
        # Run an epoch of SGD
        before_E_in = get_err(U, V, Y)
        np.random.shuffle(indices)
        for ind in indices:
            (i,j, Yij) = Y[ind]
            # Update U[i], V[j]
            U[i-1] = grad_U(U[i-1], Yij, V[:,j-1], reg, eta)
            V[:,j-1] = grad_V(V[:,j-1], Yij, U[i-1], reg, eta);
        # At end of epoch, print E_in
        E_in = get_err(U,V,Y)
        print("Epoch %s, E_in (MSE): %s"%(epoch + 1, E_in))
        
        # Compute change in E_in for first epoch
        if epoch == 0:
            delta = before_E_in - E_in
            
        # If E_in doesn't decrease by some fraction <eps>
        # of the initial decrease in E_in, stop early
        elif before_E_in - E_in < eps * delta:
            break
        
    return (U, V, get_err(U,V,Y))
